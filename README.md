# ğŸ Egyptian Hieroglyphs Classification  
_Deep Learning-based symbol recognition with CNNs and Vision Transformers_ 
> Project for the GLO-7030 Deep Learning course â€“ UniversitÃ© Laval

---

## ğŸ“Œ Overview
This project explores the classification of **Egyptian hieroglyphs** using modern **deep learning models**. We implement and evaluate both **CNNs** and **Vision Transformers (ViT)** for recognizing symbols annotated with the Gardiner sign list.

Given the challenges posed by a limited and imbalanced dataset, we investigate several training strategies, including:
- Data augmentation
- Transfer learning with pretrained models
- Fine-tuning vs. feature extraction
- An ablation study to assess each componentâ€™s impact

---

## ğŸ” Objective
Develop a robust image classifier capable of identifying Egyptian hieroglyphs, laying the groundwork for potential downstream tasks such as **translation assistance**, **theme extraction**, and **semantic reconstruction** of ancient texts.

---

## ğŸ“Š Dataset
We use the [Egyptian Hieroglyph Dataset](https://universe.roboflow.com/sameh-zaghloul/egyptianhieroglyphdataset), containing 3,584 labeled images across 170 classes. For better class balance, only the top **40 most frequent classes** were selected. All images were resized to **224Ã—224** pixels.

To improve generalization, we applied data augmentation with:
- Random rotation (Â±16Â°)
- Translation (up to 20%)
- Scaling down to 80%
- Brightness and contrast variation

---

## ğŸ§  Models Implemented

| Model                  | Pretrained | Fine-Tuned     | Accuracy | F1 Score |
|-----------------------|------------|----------------|----------|----------|
| Custom CNN            | âŒ         | â€“              | 62.7%    | 32.1%    |
| ResNet18              | âœ…         | Head only      | 98.7%    | 94.8%    |
| ResNet50              | âœ…         | Head only      | 96.2%    | 89.6%    |
| Inception-v3          | âœ…         | Head only      | 97.8%    | 91.6%    |
| ViT (from scratch)    | âŒ         | â€“              | 87.9%    | 74.2%    |
| ViT (frozen)          | âœ…         | Head only      | 91.1%    | 90.4%    |
| ViT (fine-tuned)      | âœ…         | Full model     | 94.7%    | 94.2%    |

> âœ… Best overall: **ViT pretrained with full fine-tuning**

---

## ğŸ”¬ Ablation Study
We evaluated the effect of:
- Disabling data augmentation
- Reducing the number of Transformer layers
- Adding test-time augmentation (TTA)

ğŸ§ª **Findings**:
- No augmentation led to lower F1 and recall
- Smaller models generalized poorly
- TTA failed (1.5% accuracy), likely due to implementation bugs

---

## âš™ï¸ Training Setup

- Loss: `CrossEntropyLoss`
- Optimizer: `AdamW`
- Epochs: 12â€“30 (depending on model)
- Batch size: 32 or 64
- Some models used `ReduceLROnPlateau` scheduler

---

## ğŸ‘¥ Authors
- Jade Piller-Cammal  
- Estelle Tournassat  
- ThÃ©o Parris  
- Alban Sarrazin  

---

## ğŸ“œ References
- ğŸ“š Dataset: [Roboflow â€“ Egyptian Hieroglyphs](https://universe.roboflow.com/sameh-zaghloul/egyptianhieroglyphdataset)  
- ğŸ”  Labels: [Gardiner's Sign List](https://www.egyptianhieroglyphs.net/gardiners-sign-list/)

---
